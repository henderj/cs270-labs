{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVL7_bgmIAPR"
      },
      "source": [
        "# Perceptron Lab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBIfzcfn3QPO"
      },
      "source": [
        "This is the scaffold notebook for the perceptron lab and each lab will have a similar scaffold.  Make your own copy of this notebook and then you can fill in the tasks.  **You will fill in code boxes and test boxes with discussion for most tasks** and particularly discussion of results, graphs, etc. Good writing, grammar, punctuation, etc. are important and points will be taken off if these things are lacking. \n",
        "\n",
        "In most cases just reporting results without discussion will not receive full credit. Thus, your discussion should also include sentences like the following: This is because…  Note that the …  I was surprised to observe… I am not sure why …, but my hypothesis is that …\n",
        "\n",
        "Start by looking over the scikit-learn [user guide](https://scikit-learn.org/stable/user_guide.html) and carefully reading over the [perceptron documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6ZbYjZZZ_yLV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hende\\AppData\\Local\\Temp\\ipykernel_15408\\3220352236.py:4: DeprecationWarning: \n",
            "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
            "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
            "but was not found to be installed on your system.\n",
            "If this would cause problems for you,\n",
            "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
            "        \n",
            "  import pandas as pd\n"
          ]
        }
      ],
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "from scipy.io import arff\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KibCIXIThpbE"
      },
      "source": [
        "## 1. Debug and Evaluation\n",
        "\n",
        "For most labs we will give you some data and expected results so that you can verify that you are doing things right and getting appropriate results.  We will then have you run on some other data with the exact same hyperparameters so we can see if your results are correct. \n",
        "\n",
        "### 1.1 Debug (5%)\n",
        "Download this [data set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/linsep2nonorigin.arff) into your local directory and name it debug.arff.  To download rather than view, right-click the link and save as.  To allow consistent results run the Perceptron using the following hyperparameters:\n",
        "- Learning rate = .1\n",
        "- Do NOT shuffle the data after every epoch like you would normally do\n",
        "- Learn for a maximum of 10 epochs\n",
        "- Start all weights at 0 (which is the default for this scikit-learn implementation)  \n",
        "\n",
        "Your results should be:\n",
        "Accuracy = [.875]\\\n",
        "Final Weights = [[-0.23  0.18]]\\\n",
        "[-0.1] - This last weight is the bias weight, which they call the intercept.\n",
        "\n",
        "To help you with this first task, following is one detailed variation that would do this for you. You may use your own variation.  Note that commonly used variable names in the community are clf (classifier), X (array of input features), y (vector of output labels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "KgAyy82gixIF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-0.4  0.3  1. ]\n",
            " [-0.3  0.8  1. ]\n",
            " [-0.2  0.3  1. ]\n",
            " [-0.1  0.9  1. ]\n",
            " [-0.1  0.1  0. ]\n",
            " [ 0.  -0.2  0. ]\n",
            " [ 0.1  0.2  0. ]\n",
            " [ 0.2 -0.2  0. ]]\n",
            "-- Epoch 1\n",
            "Norm: 0.04, NNZs: 2, Bias: -0.100000, T: 8, Avg. loss: 0.013500\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.07, NNZs: 2, Bias: -0.100000, T: 16, Avg. loss: 0.011875\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.11, NNZs: 2, Bias: -0.100000, T: 24, Avg. loss: 0.010250\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.14, NNZs: 2, Bias: -0.100000, T: 32, Avg. loss: 0.008625\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.18, NNZs: 2, Bias: -0.100000, T: 40, Avg. loss: 0.007000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.21, NNZs: 2, Bias: -0.100000, T: 48, Avg. loss: 0.005375\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.23, NNZs: 2, Bias: -0.100000, T: 56, Avg. loss: 0.008250\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.25, NNZs: 2, Bias: -0.100000, T: 64, Avg. loss: 0.007625\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.27, NNZs: 2, Bias: -0.100000, T: 72, Avg. loss: 0.007000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.29, NNZs: 2, Bias: -0.100000, T: 80, Avg. loss: 0.006375\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.31, NNZs: 2, Bias: -0.100000, T: 88, Avg. loss: 0.005750\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 11 epochs took 0.00 seconds\n",
            "[[-0.24  0.2 ]]\n",
            "[-0.1]\n",
            "1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\code\\school\\cs270\\cs270-labs\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "clf = Perceptron(shuffle=False, verbose=1, eta0=.1, max_iter=11)\n",
        "Data_Set = arff.loadarff('debug.arff')\n",
        "Data_Set_df = pd.DataFrame(Data_Set[0])\n",
        "Data_Set_np = Data_Set_df.to_numpy()\n",
        "Data_Set_np = Data_Set_np.astype(float)\n",
        "print(Data_Set_np)\n",
        "X = Data_Set_np[:, :-1]\n",
        "y = Data_Set_np[:, -1]\n",
        "clf.fit(X, y)\n",
        "print(clf.coef_)\n",
        "print(clf.intercept_)\n",
        "print(clf.score(X, y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notes:\n",
        "* If you increase (or just remove) max_iter your perceptron will converge to 100% in one more epoch.  Try it!\n",
        "* verbose (integer) specifies how much info you get after each epoch.  It does not change results.  Try it with 0.\n",
        "    * Norm is a measure of the total weight magnitudes in the current model.\n",
        "    * NNZs is the number of non-zero weights there currently are (not including the bias).\n",
        "    * clf.intercept_ is the value of the bias weight, NOT the actual y-intercept of the decision surface, though it will affect that.\n",
        "    * T is the total number of weight updates so far.  In this case all the weights were updated each epoch.\n",
        "    * Avg. loss is another measure of error.  More on that later.\n",
        "    * The activation function outputs 1 if net > 0, else 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY3VNB1ui03N"
      },
      "source": [
        "### 1.2 (5%) Evaluation\n",
        "\n",
        "Now run your model  on the [evaluation dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/data_banknote_authentication.arff) and report the final accuracy, weights, and bias. Use the exact same hyperparameters as the debug dataset. The most common reason for results not matching ours is not exactly using the same hyperparameters (e.g. initial weights, training exactly 10 epochs, no shuffling, etc.) Hint: Throughout these labs for the numerous cases where you are doing the same task, but with different data (e.g. debug, eval), you could create a function where you just pass the data set name, so as not to recreate the code multiple times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yAxA78QjDh2"
      },
      "outputs": [],
      "source": [
        "# Load evaluation data\n",
        "clf = Perceptron(shuffle=False, verbose=1, eta0=0.1, max_iter=10)\n",
        "Data_Set = arff.loadarff('eval.arff')\n",
        "Data_Set_df = pd.DataFrame(Data_Set[0])\n",
        "Data_Set_np = Data_Set_df.to_numpy()\n",
        "Data_Set_np = Data_Set_np.astype(float)\n",
        "X = Data_Set_np[:, :-1]\n",
        "Y = Data_Set_np[:, -1]\n",
        "# Train on the evaluation data\n",
        "clf.fit(X, Y)\n",
        "# print your accuracy and weights. Do not enter these values by hand.\n",
        "print(clf.coef_)\n",
        "print(clf.intercept_)\n",
        "print(clf.score(X, Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-4J5VcS9umq"
      },
      "source": [
        "*Discussion*  \n",
        "The perceptron converged quickly on the eval dataset compared to the debug dataset, needing only 9 epochs instead of 11. This is probably because the eval database is larger (has more rows) than the debug dataset. As such, the perceptron has more data per epoch to learn off of, and thus needs less epochs to converge. \n",
        "\n",
        "Another interesting note is that the accuracy of the perceptron on the eval dataset did not reach 100% after convergence, as it did on the debug dataset. This could be because the eval dataset is noisy, or it may not be completely linearly separable. Since it got ~97% accuracy, the dataset seems to be mostly linearly separable, although possibly a bit noisy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vWiTdlbR2Xh"
      },
      "source": [
        "## 2. Classifying on linearly separable and non-linearly separable data\n",
        "\n",
        "### 2.1 (5%) Create 2 datasets\n",
        "\n",
        "- Both with 8 instances using 2 real valued inputs with 4 instances from each class. \n",
        "- One data set should be linearly separable and the other not.\n",
        "- Show or print your datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SSoasDQSKXb"
      },
      "outputs": [],
      "source": [
        "# Create and show the 2 datasets.\n",
        "# You could do it here in Python or create arff files in your local folder with a text editor.  Either way show your dataset.\n",
        "\n",
        "dataset_1 = np.array(\n",
        "    [\n",
        "        [1.0, 5.5, 1],\n",
        "        [1.6, 5.1, 1],\n",
        "        [1.2, 4.9, 1],\n",
        "        [2.1, 4.2, 1],\n",
        "        [3.7, 2.5, 0],\n",
        "        [4.5, 1.4, 0],\n",
        "        [3.9, 0.8, 0],\n",
        "        [5.8, 1.6, 0],\n",
        "    ]\n",
        ")\n",
        "\n",
        "dataset_2 = np.array(\n",
        "    [\n",
        "        [1.7,2.8,1],\n",
        "        [3.1,3.9,1],\n",
        "        [3.9,1.4,1],\n",
        "        [5.0,5.1,1],\n",
        "        [1.5,1.5,0],\n",
        "        [1.6,4.1,0],\n",
        "        [2.5,2.5,0],\n",
        "        [3.8,3.2,0]\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"**Dataset 1 (linearly separable**\")\n",
        "print(dataset_1)\n",
        "print(\"**Dataset 2 (non-linearly separable)\")\n",
        "print(dataset_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIRG42TgSR4x"
      },
      "source": [
        "### 2.2 (5%) Train on both sets using the scikit-learn perceptron model\n",
        "- Train on each and print the results. Train until convergence (i.e. use the default max_iter by simply not specifying it).\n",
        "- You may choose your other hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBGUn43ASiXW"
      },
      "outputs": [],
      "source": [
        "# Train and print results with both datasets\n",
        "\n",
        "print(\"**Dataset 1**\")\n",
        "clf1 = Perceptron(eta0=0.1)\n",
        "X1 = dataset_1[:, :-1]\n",
        "Y1 = dataset_1[:, -1]\n",
        "clf1.fit(X1, Y1)\n",
        "print(clf1.coef_)\n",
        "print(clf1.intercept_)\n",
        "print(clf1.score(X1, Y1))\n",
        "\n",
        "print(\"\\n**Dataset 2**\")\n",
        "clf2 = Perceptron(eta0=0.1)\n",
        "X2 = dataset_2[:, :-1]\n",
        "Y2 = dataset_2[:, -1]\n",
        "clf2.fit(X2, Y2)\n",
        "print(clf2.coef_)\n",
        "print(clf2.intercept_)\n",
        "print(clf2.score(X2, Y2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "890A39KX9zLC"
      },
      "source": [
        "*Discussion*\n",
        "\n",
        "The perceptron converged with an accuracy of 100% on the first dataset. This is likely because it is linearly separable. It surprised me that the bias on the first dataset ended up being 0. I'm not sure what made it 0 while most other datasets end up with a non-zero bias. Maybe this was just a simpler dataset. Having a bias of 0 will also make the y-intercept of the separation line be 0, so only the slope will matter in this case.\n",
        "\n",
        "On the other hand, the perceptron converged with an accuracy of only 50% on the second dataset. The second dataset is not linearly separable, so it is impossible for the perceptron to get 100% accuracy on the dataset without any feature engineering. It may be possible to gain 100% accuracy with feature engineering, but that is not part of this lab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v19fpixqTe-7"
      },
      "source": [
        "### 2.3 (10%) Graph the datasets and their corresponding decision line\n",
        " \n",
        " - Graph each dataset\n",
        " - Use your trained perceptrons above to determine each dataset's decision line\n",
        "    - We discssed how to find the slope and intercept in class and the equation is in the Percptron slides\n",
        " - For all graphs always label the axes!\n",
        " - Following are examples of what graphs could look like\n",
        " \n",
        "![Linearly Separable Data](https://raw.githubusercontent.com/rmorain/CS472-1/master/images/perceptron/linearly_separable.png)\n",
        "\n",
        "![Not Linearly Separable](https://raw.githubusercontent.com/rmorain/CS472-1/master/images/perceptron/not_linearly_separable.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCPFUAGTS2sX"
      },
      "outputs": [],
      "source": [
        "# Graph datasets and decision lines for both cases.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dataset 1\n",
        "data = X1\n",
        "labels = Y1\n",
        "\n",
        "plt.xlim(0,6)\n",
        "plt.ylim(0,6)\n",
        "scatter = plt.scatter(data[:,0], data[:,1], c = labels, s=200, cmap='seismic')\n",
        "legend = plt.legend(*scatter.legend_elements(num=1), title=\"Class\", loc='upper right') # Legend\n",
        "\n",
        "xlist = np.linspace(0, 6.0, 100) \n",
        "ylist = np.linspace(0, 6.0, 100) \n",
        "slope = (-clf1.coef_[0][0]/clf1.coef_[0][1])\n",
        "intercept = -(clf1.intercept_/clf1.coef_[0][1])\n",
        "eq = slope*xlist + intercept\n",
        "plt.plot(xlist,  eq, '-k', )\n",
        "plt.title('Linearly Separable Data Example')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.grid() \n",
        "plt.show() \n",
        "\n",
        "\n",
        "# Dataset 2\n",
        "data = X2\n",
        "labels = Y2\n",
        "\n",
        "plt.xlim(0,6)\n",
        "plt.ylim(0,6)\n",
        "scatter = plt.scatter(data[:,0], data[:,1], c = labels, s=200, cmap='seismic')\n",
        "legend = plt.legend(*scatter.legend_elements(num=1), title=\"Class\", loc='upper right') # Legend\n",
        "\n",
        "xlist = np.linspace(0, 6.0, 100) \n",
        "ylist = np.linspace(0, 6.0, 100) \n",
        "slope = (-clf2.coef_[0][0]/clf2.coef_[0][1])\n",
        "intercept = -(clf2.intercept_/clf2.coef_[0][1])\n",
        "eq = slope*xlist + intercept\n",
        "plt.plot(xlist,  eq, '-k', )\n",
        "plt.title('Linearly Separable Data Example')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.grid() \n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7dyHR0c92zd"
      },
      "source": [
        "*Discussion*\n",
        "\n",
        "It's amusing that the perceptron converged to the line that it did for the second dataset. The second dataset is non-linearly separable, so the perceptron wasn't going to be able to perfectly separate the data. What's funny is that the perceptron did not separate any data. The line it produced has all of the data on one side and none of the data on the other side. Perhaps this is the highest accuracy it can get. Since it classifies all data as the same class, and there are 5 data points for each class, the perceptron got exactly 50% accuracy by simply classifying all of the training data the same way. This is not any better than randomly guessing which data point is in which class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N5RXV8AzdRF"
      },
      "source": [
        "### 2.4 (5%) Discuss why perceptrons in general will not get perfect accuracy on non-linearly separable data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hSjHuLdzdRF"
      },
      "source": [
        "*Discussion*\n",
        "\n",
        "Perceptrons learn to classify data by using a form of linear regression and finding a line the separates the data. By nature, perceptrons will have difficulty getting perfect accuracy on non-linearly separable data. Non-linearly separable data is data that cannot be separated by a line (or a plane/hyperplane in higher dimensions). Since the data cannot be separated with a line, the perceptron generally cannot get perfect accuracy on non-linearly separable data.\n",
        "\n",
        "There are, however, strategies and methods that allow perceptrons to accurately classify non-linearly separable data. By using feature engineering, we can simulate higher dimensions to a dataset. This can sometimes allow the perceptron to correctly separate the data by using the extra data provided by the feature engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ySvhd1lUGSe"
      },
      "source": [
        "## 3. Use the perceptron to learn this version of the [voting data set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/voting-dataset.arff)\n",
        "\n",
        "This particular task is an edited version of the standard voting set, where we have replaced all the “don’t know” values with the most common value for the particular attribute. Look at the [tutorial](https://github.com/cs472ta/CS472/blob/master/Tutorial.ipynb) from the Labs Content page to see examples of prepping the voting data set for sklearn.\n",
        "\n",
        "### 3.1 (10%) Average final training and test set accuracy over multiple trials\n",
        "\n",
        "- Learn the voting data five times with different random 70/30 Training/Test splits each time\n",
        "- Use a learning rate of 1.  The other hyperparameters are up to you.\n",
        "- Report the 5 trials and the average training and test accuracy and number of epochs to converge across the 5 trials in a table \n",
        "    - Below is an example of what a 2 trial table might look like\n",
        "\n",
        "| Trial | Training Accuracy | Test accuracy | Number of epochs |\n",
        "| --- | --- | --- | --- |\n",
        "| 1 | .950 | .550 | 4 |\n",
        "| 2 | .850 | .450 | 6 |\n",
        "| Average | .900 | .500 | 5 | \n",
        "\n",
        "*- As a rough sanity check, typical Perceptron test accuracies for the voting data set are 90%-98%.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Romyl4G8Trki"
      },
      "outputs": [],
      "source": [
        "# Run the perceptron 5 times on the voting data set with different splits and record the results.\n",
        "# Hint: look up scikit-learn train_test_split\n",
        "# Create the table using the tool of your choice\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def do_split_training(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "    perceptron = Perceptron(eta0=1, verbose=1)\n",
        "    perceptron.fit(X_train, y_train)\n",
        "    print(perceptron.score(X_train, y_train), perceptron.score(X_test, y_test))\n",
        "\n",
        "voting_dataset_arff = arff.loadarff(\"voting-dataset.arff\")\n",
        "voting_dataset = pd.DataFrame(voting_dataset_arff[0])\n",
        "voting_dataset = voting_dataset.apply(LabelEncoder().fit_transform)\n",
        "\n",
        "X_voting = voting_dataset.iloc[:, :-1]\n",
        "y_voting = voting_dataset.iloc[:, -1]\n",
        "\n",
        "do_split_training(X_voting, y_voting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print((.922+.956+.95+.969+.954)/5)\n",
        "print((.913+.964+.935+.914+.961)/5)\n",
        "print((9+12+21+14+7)/5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Trial | Training accuracy | Test accuracy | Number of epochs |\n",
        "| ----- | ----------------- | ------------- | ---------------- |\n",
        "| 1 | .922 | .913 | 9 |\n",
        "| 2 | .956 | .964 | 12 |\n",
        "| 3 | .950 | .935 | 21 |\n",
        "| 4 | .969 | .914 | 14 |\n",
        "| 5 | .954 | .961 | 7 |\n",
        "| Average | .950 | .937 | 12.6 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm4rbWkzVeoR"
      },
      "source": [
        "### 3.2 (10%) Interpreting the Model\n",
        "- Learn with the perceptron once on the full voting data set\n",
        "- Look at the learned weights and bias and do your best to explain what the model has learned and how the individual input features affect the result. \n",
        "- Which specific features are most critical for the voting task, and which are least critical? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Learn once on the full voting data set.  If you Look at our tutorial at the beginning \n",
        "\n",
        "perceptron = Perceptron(verbose=1)\n",
        "perceptron.fit(X_voting, y_voting)\n",
        "print(perceptron.coef_)\n",
        "print(perceptron.intercept_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "voting_dataset.columns[14]\n",
        "# voting_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Discuss how the different features effect prediction\n",
        "\n",
        "We can tell which features effect the prediction the most by looking at the magnitudes of their respective weights. This is because the inputs will be multiplied by the weights, and then added together to determine the output. So a higher weight on a given feature will have a greater effect on the output. Likewise, the lowest magnitude weights will have the least amount of effect on the output.\n",
        "\n",
        "For the voting dataset, the highest feature by far is the fourth one, `physician-fee-freeze`, with a weight of `17`. Since the weight is positive, the presence of this feature will be highly correlated with the output of `1`, or `republican`. This suggests that most republican representatives voted for `physician-fee-freeze`.\n",
        "\n",
        "The second highest feature weight is the 11th: `synfuels-corporation-cutback`. This weight is `-10`, which is negative, and so it will correlate with the output of `0`, or `democrat`. This suggests that most democrat representatives voted for `synfuels-corporation-cutback`.\n",
        "\n",
        "There are 2 features with weights of `0`: the first one, `handicapped-infants`; and the 14th one, `crime`. With weights of `0`, these 2 features have no effect on the output. This suggests that both democrat and republican representatives were equally likely to vote either for or against `handicapped-infants` and `crime`. In other words, if you knew that a certain representative had either voted for or against `handicapped-infants` or `crime`, you would not be able to determine if that representative was a democrat or a republican.\n",
        "\n",
        "The other features have weights with varying magnitudes and signs. We could possibly use this information to see which policies democrat and republican representatives voted for in general. However, with such a high negative bias (`-9`), it is difficult to accurately determine which policy each party voted for in general, especially for the features with low-magnitude weights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyUmJ4yWVsQ7"
      },
      "source": [
        "### 3.3 (15%) Misclassification rate vs epochs \n",
        "\n",
        "Learn with the perceptron once on the full voting data set and graph the misclassification rate (number_misclassified  /  number_of_total_data_points) at each epoch until convergence (0th – final epoch).  Hints:\n",
        "- For the 0th epoch (i.e. initial weights), assume the initial misclassification rate is 50% and let that be the first point on your graph. Since there are 2 output classes, random guessing would only get it right about 1/2 of the time. Missclassifcation rate is 1 - accuracy.\n",
        "- Doing clf.fit with max_iter = 1, and warm_start=True will let you loop through epochs one at a time, allowing you to look at accuracy, etc. after each epoch.  But how do you know how many epochs to run until convergence?  One way is to just to first run it until convergence with random_state = c to see how many epochs it took (n_iter_).  Then run it again one epoch at a time n_iter_ times using the same random_state constant c (so that you get the exact same conditions).\n",
        "- Below is a rough example of what this type of graph might look like (though this graph is for avg rate across multiple trials)\n",
        "\n",
        "![Average Misclassification Rate](https://raw.githubusercontent.com/rmorain/CS472-1/master/images/perceptron/avg_misclassification_rate_vs_epochs.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_ZRPWWVVeFM"
      },
      "outputs": [],
      "source": [
        "# Learn once on the full voting data set, while gathering info after every epoch\n",
        "# Then Graph it here.  Here is some sample graph code, but you can do it your own way, including using a bar graph, etc.\n",
        "    \n",
        "def plot_misclassification(misclassification_rate):\n",
        "    \"\"\"\n",
        "        Plots the misclassification rate\n",
        "    Args:\n",
        "        misclassification_rate (array-like): A 1D array or list of missclassifcation rate at every epoch\n",
        "    \"\"\"\n",
        "    plt.plot(np.arange(len(misclassification_rate)), misclassification_rate)\n",
        "    plt.title(\"Misclassification Rate vs. Epoch\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Misclassification Rate\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_state = 31\n",
        "num_iter = 10\n",
        "misclassification_rate = [0.5]\n",
        "# perceptron = Perceptron(random_state=31, verbose=1)\n",
        "perceptron = Perceptron(max_iter=1, warm_start=True, random_state=31)\n",
        "for i in range(0, num_iter):\n",
        "    perceptron.fit(X_voting, y_voting)\n",
        "    misclassification_rate.append(1-perceptron.score(X_voting, y_voting))\n",
        "plot_misclassification(misclassification_rate)\n",
        "print(misclassification_rate[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgqUrgh--GeU"
      },
      "source": [
        "*Show table and discuss*\n",
        "\n",
        "The misclassification rate begins at 0.5 and quickly moves down to below 0.1. After convergence, the misclassification rate is at ~0.0369. This sharp decrease in the misclassification rate makes sense, because the perceptron quickly learns which direction to push its weights. After 4 epochs, the misclassification rate stays pretty consistently around 0.03-0.04. This makes sense because by this point the perceptron has figured out the approximate true values of the weights. What surprised me is that there is a little jump in the misclassification rate at the 9th epoch. This could be because the weights were overshot a bit. They were going in the right direction, but they might have moved a little too much. In the 10th epoch, the misclassification rate was back down to it's previous value. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 (15%) Hyperparameters\n",
        "Read over all the Hyperparameters and their defaults in the scikit [perceptron documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html). With the voting data set, experiment briefly with EACH of the following 6 hyperparameters and discuss your findings.\n",
        "- shuffle\n",
        "- eta0 - learning rate\n",
        "- verbose\n",
        "- fit_intercept - whether to use a bias weight or not\n",
        "- random state\n",
        "- warm start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the model with different hyperparameters\n",
        "\n",
        "def do_training(perceptron):\n",
        "    perceptron.fit(X_voting, y_voting)\n",
        "    print(perceptron.coef_)\n",
        "    print(perceptron.intercept_)\n",
        "    print(perceptron.score(X_voting, y_voting))\n",
        "\n",
        "perceptron1 = Perceptron(warm_start=False)\n",
        "perceptron2 = Perceptron(warm_start=True)\n",
        "# perceptron3 = Perceptron(random_state=11)\n",
        "\n",
        "do_training(perceptron1)\n",
        "do_training(perceptron2)\n",
        "do_training(perceptron2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Discussion\n",
        "\n",
        "### shuffle\n",
        "\n",
        "With shuffle turned off, the accuracy on the dataset is slightly higher. This could be because the perceptron is memorizing the dataset instead of generalizing. In addition, many of the weights are different, but not significantly. Most of the signs stayed the same (except for the ones in the range -1 to 1), and most of the weights stayed within ±2 integer points. It's not a huge difference because the weights still go in the same direction with shuffle turned off, they just are slightly more or less extreme.\n",
        "\n",
        "### eta0\n",
        "\n",
        "I tested with eta0=1, eta0=0.1, and eta0=2. eta0=1 is the default and gave the normal weights and bias. The other two were ended up with weights that were approximately multiples of the normal weights and bias. With eta0=2, the weights and bias were exactly double that of the weights and bias with eta0=1. This works, because the weights will push the output twice as far, but the output has to go twice as far for it to change because the bias is also twice as large. With eta0=0.1, a few of the weights were exactly one-tenth of the eta0=1 ones, but many were slightly different. This could be because the weights are taking smaller steps, and so they reach different optimal points. In all three cases, the accuracy was the exact same. With this particular dataset and model, the learning rate does not seem to have a significant effect on the accuracy.\n",
        "\n",
        "### verbose\n",
        "\n",
        "The only numbers that seem to mean anything for the verbose hyperparameter are 0 and 1. I tested with numbers greater than 1, but the output stayed the same. With a value of 0, the .fit method does not print anything. With a value of 1, the .fit method prints out information after each epoch. The value of verbose does not effect the outcome of the weights, bias, or accuracy. This makes sense, since logging information should have no effect on the mathematical process.\n",
        "\n",
        "### fit_intercept\n",
        "\n",
        "The default for fit_intercept is True. This results in the normal weights. Setting fit_intercept to False results in different weights and a bias of 0. This makes sense, as the bias is directly related to the y-intercept of the resulting separation line (or hyperplane, since the dimension of this data is greater than 3). If the perceptron should not move the intercept, then the bias should stay at 0. With fit_intercept set to False, the weights are different, but generally have the same sign as when fit_intercept is True. This makes sense, as the weights effect the slope of the line, so slightly different weights will mean a slightly different slope to compensate for the intercept staying at the origin. In addition, the accuracy is slightly less with fit_intercept set to False. This is likely because the perceptron has one less weight to adjust, so it can't get as accurate.\n",
        "\n",
        "### random_state\n",
        "\n",
        "According to the documentation, random_state effects the shuffling of the data when shuffle=True. I tested with values of 0, 371, and 11. With random_state=0, the weights and bias were the same as the normal weights and bias. This makes sense because the default value for random_state is 0. With random_state=371, the weights were different, but generally not significantly different. The bias was increased by one, but the accuracy stayed the same. This is likely the effect of the data being shuffled differently after each epoch. With a different data order, the weights learn on the same data but in a different order. With random_state=11, the weights and bias were again slightly different, but in this case the accuracy was a whole percentage point better. It surprised me that the accuracy would increase by that much by simply changing the random state, but it did. Maybe another value for random_state would result in an even higher accuracy rate. \n",
        "\n",
        "### warm_start\n",
        "\n",
        "The default for warm_start is False, so it was no surprise that setting warm_start to False resulted in the normal weights and bias. Setting warm_start to True and running .fit initially resulted in the same weights and bias as well. However, I called .fit on the perceptron again, and this time it kept training. The resulting weights and bias were very similar to the normal ones, but they were slightly different. It was somewhat surprising to note that the accuracy dropped down after training again. However, after further thought, this made sense. After the first call to .fit, the weights had converged to values that got a fairly high accuracy rate. Training them again did not increase the accuracy; the weights were already at (or at least very close to) a local minimum. The further training simply pushed the weights a little bit too far, so the accuracy dipped down a bit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxssT08TzdRH"
      },
      "source": [
        "## 4 (15%) Use the perceptron to learn one other data set of your choice.  \n",
        "- The UC Irvine Data Repository is one great source, but you may get your data set from wherever you like, though it should be a real world task.\n",
        "- Report your results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqPZLMf1zdRH"
      },
      "outputs": [],
      "source": [
        "# Load and Train on your dataset and report results\n",
        "\n",
        "weather_df = pd.read_csv(\"./weatherAUS.csv\")\n",
        "weather_df_backup = weather_df.copy()\n",
        "\n",
        "# drop rows without target value or RainToday value\n",
        "weather_df.dropna(subset=['RainToday', 'RainTomorrow'], inplace=True) \n",
        "\n",
        "target_col = ['RainTomorrow']\n",
        "number_cols = ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm']\n",
        "category_cols = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']\n",
        "\n",
        "# drop these columns because we only have about half the data\n",
        "number_cols = [x for x in number_cols if x not in ['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm']] \n",
        "\n",
        "# use mean values to populate missing values\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy=\"mean\")\n",
        "imputer.fit(weather_df[number_cols])\n",
        "weather_df[number_cols] = imputer.transform(weather_df[number_cols])\n",
        "\n",
        "# scale values to be from 0 to 1\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(weather_df[number_cols])\n",
        "\n",
        "weather_df[number_cols] = scaler.transform(weather_df[number_cols])\n",
        "\n",
        "# convert the categorical data into one-hot features\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "encoder.fit(weather_df[category_cols])\n",
        "\n",
        "encoded_cols = list(encoder.get_feature_names_out(category_cols))\n",
        "weather_df[encoded_cols] = encoder.transform(weather_df[category_cols])\n",
        "\n",
        "# train perceptron\n",
        "weather_train_X, weather_test_X, weather_train_y, weather_test_y = train_test_split(\n",
        "    weather_df[number_cols + encoded_cols], weather_df[target_col], test_size=0.3\n",
        ")\n",
        "\n",
        "weather_perceptron = Perceptron()\n",
        "weather_perceptron.fit(weather_train_X, weather_train_y)\n",
        "\n",
        "print(weather_perceptron.coef_)\n",
        "print(weather_perceptron.intercept_)\n",
        "print(weather_perceptron.score(weather_test_X, weather_test_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqSFAXwlk3Ms"
      },
      "source": [
        "*Discuss how the perceptron did on the data set and why you think it performed as such.*\n",
        "\n",
        "I got data from kaggle.com that has climate information from Australia and then a target column on whether or not it rained the next day. My goal was to train the perceptron to predict whether or not it would rain the next day based on the recorded climate data for the current day. I had to do some data cleaning and encoding to get it ready for the perceptron. Four of the features were missing over 40% of their values, so I just dropped those features.\n",
        "\n",
        "I split the data into 70% training data and 30% test data. I trained the perceptron on the training data and then scored it on the test data. It got an accuracy of 81.95%. While this looks pretty good, about 75% of the rows had 'RainTomorrow' as 'No', so this model doesn't do much better than a model that just outputs 'No' all the time. However, it does do slightly better, so it's not completely useless.\n",
        "\n",
        "Although the perceptron seemed to have learned something, it still did not become an oracle that could confidently declare whether or not there will be rain tomorrow. This is likely because the weather is a difficult thing to predict. Even the best weather predicting models out there are frequently wrong (based on personal anecdotal evidence). \n",
        "\n",
        "Still, 81.95% isn't bad, and with better data, data cleaning, and a better model, I wouldn't be surprised if the accuracy increased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTlK-kijk8Mg"
      },
      "source": [
        "## 5. (Optional 15% extra credit) Code up your own perceptron learner \n",
        "\n",
        "Past versions of this class required students to code up their own version of each model and then compare results with scikit-learn.  Actually coding a model leads to better understanding of what is really happening \"under the hood.\"  Though not required for this class, we give a significant extra credit reward for each lab to those who choose to code their own models.  If you choose to do this, your code must be your own, and NOT taken from any other source (including the text).  Once you are done coding we encourage you to look at other versions to see how yours compares.  Below is a scaffold you could use if you want. Requirements for this task:\n",
        "- Your perceptron should support the methods shown in the example scaffold below.\n",
        "- Your output node should output 1 if net is > 0, else 0.\n",
        "- Use Stochastic/On-line training updates: Iterate and update weights after each training instance (i.e. not batch updates)\n",
        "- Implement a stopping criteria: when your model has trained for a number of epochs with no improvement in accuracy, stop training. Note that the weights/accuracy do not usually change monotonically.\n",
        "- Your class can inherit from the relevant scikit-learn learners (e.g. data shuffling, etc.), but don't call any of the super methods to accomplish the core methods in the scaffold.\n",
        "- Run the evaluation data set and the voting data set above with your Perceptron version. Show and discuss your results and how they compare with the scikit-learn version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iY77P7gk1Nh"
      },
      "outputs": [],
      "source": [
        "# Your Perceptron Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCMC9aHQ-IGL"
      },
      "source": [
        "*Discussion*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional scaffold for Extra Credit Perceptron code.  IF you use this feel free to change/add any methods you want.\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "import numpy as np\n",
        "class PerceptronClassifier(BaseEstimator,ClassifierMixin):\n",
        "\n",
        "    def __init__(self, lr=.1, shuffle=True):\n",
        "        \"\"\" \n",
        "            Initialize class with chosen hyperparameters.\n",
        "        Args:\n",
        "            lr (float): A learning rate / step size.\n",
        "            shuffle: Whether to shuffle the training data each epoch. DO NOT \n",
        "            SHUFFLE for evaluation / debug datasets.\n",
        "        \"\"\"\n",
        "        self.lr = lr\n",
        "        self.shuffle = shuffle\n",
        "        self.weights = np.array([])\n",
        "\n",
        "    def fit(self, X, y, initial_weights=None):\n",
        "        \"\"\" \n",
        "            Fit the data; run the algorithm and adjust the weights to find a \n",
        "            good solution\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data, excluding\n",
        "            targets\n",
        "            y (array-like): A 1D numpy array with the training targets\n",
        "            initial_weights (array-like): allows the user to provide initial \n",
        "            weights\n",
        "        Returns:\n",
        "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
        "        \"\"\"\n",
        "        \n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "\n",
        "        self.weights = self.initialize_weights(X.shape[1]) if not initial_weights else initial_weights\n",
        "\n",
        "        no_improvement_count = 0\n",
        "        no_improvement_threshold = 2\n",
        "        improvement_threshold = 0.00001\n",
        "        max_accuracy = self.score(X, y)\n",
        "        count_epochs = 0\n",
        "        while no_improvement_count <= no_improvement_threshold:\n",
        "            self.train_one_epoch(X, y)\n",
        "            accuracy = self.score(X, y)\n",
        "            improvement = accuracy - max_accuracy\n",
        "            if improvement < improvement_threshold:\n",
        "                no_improvement_count += 1\n",
        "            else:\n",
        "                no_improvement_count = 0\n",
        "            count_epochs += 1\n",
        "\n",
        "        print(\"epochs: \" + str(count_epochs))\n",
        "\n",
        "        return self\n",
        "\n",
        "    def train_one_epoch(self, X, y):\n",
        "        X = np.column_stack((X, np.ones(X.shape[0])))\n",
        "        for i in range(X.shape[0]):\n",
        "            x = X[i]\n",
        "            z = x @ self.weights\n",
        "            delta_w = self.lr * (y[i] - z) * x\n",
        "            self.weights = self.weights + delta_w\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\" \n",
        "            Predict all classes for a dataset X\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data, excluding \n",
        "            targets\n",
        "        Returns:\n",
        "            array, shape (n_samples,)\n",
        "                Predicted target values per element in X.\n",
        "        \"\"\"\n",
        "        X = np.array(X)\n",
        "        X = np.column_stack((X, np.ones(X.shape[0])))\n",
        "        y = [1 if (x @ self.weights) > 0 else 0 for x in X]\n",
        "        return y\n",
        "\n",
        "    def initialize_weights(self, n: int):\n",
        "        \"\"\" Initialize weights for perceptron. Don't forget the bias!\n",
        "        Returns: initialized weights\n",
        "        \"\"\"\n",
        "        weights = [0 for i in range(n + 1)]\n",
        "        self.weights = np.array(weights)\n",
        "        return self.weights\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\" \n",
        "            Return accuracy of model on a given dataset. Must implement own \n",
        "            score function.\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with data, excluding targets\n",
        "            y (array-like): A 1D numpy array with targets\n",
        "        Returns:\n",
        "            score : float\n",
        "                Mean accuracy of self.predict(X) wrt. y.\n",
        "        \"\"\"\n",
        "        y = np.array(y)\n",
        "        y_predicted = self.predict(X)\n",
        "        num_correct = len([v for i,v in enumerate(y) if y[i] == y_predicted[i]])\n",
        "        return num_correct / y.shape[0]\n",
        "\n",
        "    ### Returns the weights and bias. Not required but great for debugging.\n",
        "    def get_weights(self):\n",
        "        return self.weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epochs: 3\n",
            "[-0.20127351  0.36624031  0.30523317]\n",
            "0.5\n"
          ]
        }
      ],
      "source": [
        "test_perceptron = PerceptronClassifier()\n",
        "test_perceptron.initialize_weights(3)\n",
        "test_perceptron.fit(X, y)\n",
        "print(test_perceptron.weights)\n",
        "print(test_perceptron.score(X, y))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 2.7.16 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
