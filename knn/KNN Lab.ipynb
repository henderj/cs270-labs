{"cells":[{"cell_type":"markdown","metadata":{"id":"DVL7_bgmIAPR"},"source":["# K-Nearest Neighbor Lab\n","Read over the sklearn info on [nearest neighbor learners](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification)\n","\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10944,"status":"ok","timestamp":1710473954899,"user":{"displayName":"Jacob Cahoon","userId":"04411591980369705380"},"user_tz":360},"id":"6ZbYjZZZ_yLV","outputId":"b3bd5508-87a3-4d82-ad4d-f2ba36a8f7a2"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\hende\\AppData\\Local\\Temp\\ipykernel_20128\\1745216034.py:3: DeprecationWarning: \n","Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n","(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n","but was not found to be installed on your system.\n","If this would cause problems for you,\n","please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n","        \n","  import pandas as pd\n"]}],"source":["from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n","import numpy as np\n","import pandas as pd\n","from scipy.io import arff\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","try:\n","    from CS270Boi.discussion270 import Discussion\n","except:\n","    !pip install -U -q CS270Boi\n","    from CS270Boi.discussion270 import Discussion"]},{"cell_type":"markdown","metadata":{"id":"jzhgOPXrLELO"},"source":["## 1.0 (0%) Set `net_id` to Your NetID"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"TVfn_H9PLELO"},"outputs":[],"source":["# This should match your BYU email.\n","# For example, if my BYU email were jake270@byu.edu, I would set net_id to \"jake270\"\n","\n","net_id = \"joshhend\""]},{"cell_type":"markdown","metadata":{"id":"kqG5yDAHLELP"},"source":["# --------------------_Make sure to run all of the cells before continuing_--------------------\n","### The discussions and text box are loaded in by running the cell associated with the discussion.\n","### If you experience any problems/errors with the discussions, please send Jake Cahoon (TA) a message on Discord :)"]},{"cell_type":"markdown","metadata":{"id":"BuILPEKHLELP"},"source":["## 1 K-Nearest Neighbor (KNN) algorithm\n","\n","### 1.1 (15%) Basic KNN Classification\n","\n","Learn the [Glass data set](https://archive.ics.uci.edu/dataset/42/glass+identification) using [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) with default parameters.\n","- Randomly split your data into train/test.  Anytime we don't tell you specifics (such as what percentage is train vs test) choose your own reasonable values\n","- Give typical train and test set accuracies after running with different random splits\n","- Print the output probabilities for a test set (predict_proba)\n","- Try it with different p values (Minkowskian exponent) and discuss any differences"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":444},"executionInfo":{"elapsed":117,"status":"ok","timestamp":1710473988896,"user":{"displayName":"Jacob Cahoon","userId":"04411591980369705380"},"user_tz":360},"id":"ihgbxcJBMMua","outputId":"00175d4c-484f-4317-d668-085c80c288cf"},"outputs":[],"source":["headers = ['ID', 'RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe', 'Class']\n","df = pd.read_csv(\"glass.csv\", header=None, names=headers)\n","df = df.drop('ID', axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Preprocessing\n","from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler()\n","df_scaled = pd.DataFrame(scaler.fit_transform(df.iloc[:, :-1]), columns=df.columns[:-1])\n","df_scaled[df.columns[-1]] = df[df.columns[-1]]\n","df_scaled"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"l5lXsB3SLELP"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Accuracy:  0.7192982456140351\n","Test Accuracy:  0.7674418604651163\n","Probabilites:  [[0.  1.  0.  0.  0.  0. ]\n"," [0.6 0.4 0.  0.  0.  0. ]\n"," [0.4 0.2 0.4 0.  0.  0. ]\n"," [0.  0.  0.  0.6 0.2 0.2]\n"," [0.2 0.2 0.6 0.  0.  0. ]\n"," [0.  1.  0.  0.  0.  0. ]\n"," [0.4 0.4 0.2 0.  0.  0. ]\n"," [1.  0.  0.  0.  0.  0. ]\n"," [0.8 0.  0.2 0.  0.  0. ]\n"," [0.  0.6 0.  0.4 0.  0. ]\n"," [0.  0.  0.  0.  0.  1. ]\n"," [0.2 0.8 0.  0.  0.  0. ]\n"," [0.  1.  0.  0.  0.  0. ]\n"," [0.  0.6 0.  0.2 0.2 0. ]\n"," [0.  0.  0.  0.  0.  1. ]\n"," [0.6 0.4 0.  0.  0.  0. ]\n"," [0.2 0.6 0.2 0.  0.  0. ]\n"," [0.2 0.8 0.  0.  0.  0. ]\n"," [1.  0.  0.  0.  0.  0. ]\n"," [0.  0.4 0.  0.6 0.  0. ]\n"," [0.4 0.4 0.2 0.  0.  0. ]\n"," [0.  0.  0.  0.  0.  1. ]\n"," [1.  0.  0.  0.  0.  0. ]\n"," [0.  0.  0.  0.  0.  1. ]\n"," [0.  1.  0.  0.  0.  0. ]\n"," [0.6 0.2 0.2 0.  0.  0. ]\n"," [0.6 0.2 0.2 0.  0.  0. ]\n"," [0.  0.8 0.2 0.  0.  0. ]\n"," [1.  0.  0.  0.  0.  0. ]\n"," [0.  0.  0.  0.  0.  1. ]\n"," [0.6 0.2 0.2 0.  0.  0. ]\n"," [0.  1.  0.  0.  0.  0. ]\n"," [0.  0.6 0.  0.4 0.  0. ]\n"," [0.  0.6 0.  0.  0.4 0. ]\n"," [0.2 0.  0.  0.2 0.6 0. ]\n"," [0.8 0.2 0.  0.  0.  0. ]\n"," [0.6 0.2 0.2 0.  0.  0. ]\n"," [1.  0.  0.  0.  0.  0. ]\n"," [0.  0.6 0.4 0.  0.  0. ]\n"," [0.  1.  0.  0.  0.  0. ]\n"," [1.  0.  0.  0.  0.  0. ]\n"," [0.2 0.8 0.  0.  0.  0. ]\n"," [0.  0.6 0.  0.4 0.  0. ]]\n"]}],"source":["# Learn the glass data\n","target_col = 'Class'\n","X_train, X_test, y_train, y_test = train_test_split(df_scaled.drop(target_col, axis=1), df_scaled[target_col], test_size=0.2)\n","\n","# KNN Classifier\n","knn = KNeighborsClassifier()\n","knn.fit(X_train, y_train)\n","\n","print(\"Train Accuracy: \", knn.score(X_train, y_train))\n","print(\"Test Accuracy: \", knn.score(X_test, y_test))\n","print(\"Probabilites: \", knn.predict_proba(X_test))\n"]},{"cell_type":"markdown","metadata":{},"source":["num | train accuracy | test accuracy\n","--- | --- | ---\n","1 | 0.766 | 0.581\n","2 | 0.760 | 0.581\n","3 | 0.754 | 0.698\n","-- | -- | --\n","avg | 0.760 | 0.620"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["| p | Train Accuracy | Test Accuracy |\n","|---|----------------|---------------|\n","| 1 | 0.725 | 0.837 |\n","| 1.5 | 0.708 | 0.791 |\n","| 2 | 0.719 | 0.767 |\n","| 2.5 | 0.731 | 0.767 |\n","| 3 | 0.737 | 0.767 |\n","| 3.5 | 0.725 | 0.744 |\n","| 4 | 0.719 | 0.721 |\n"]}],"source":["p_values = [1, 1.5, 2, 2.5, 3, 3.5, 4]\n","print(\"| p | Train Accuracy | Test Accuracy |\")\n","print(\"|---|----------------|---------------|\")\n","for p in p_values:\n","    knn = KNeighborsClassifier(p=p)\n","    knn.fit(X_train, y_train)\n","    print(f\"| {p} | {knn.score(X_train, y_train):.3f} | {knn.score(X_test, y_test):.3f} |\")"]},{"cell_type":"markdown","metadata":{},"source":["| p | Train Accuracy | Test Accuracy |\n","|---|----------------|---------------|\n","| 1 | 0.725 | 0.837 |\n","| 1.5 | 0.708 | 0.791 |\n","| 2 | 0.719 | 0.767 |\n","| 2.5 | 0.731 | 0.767 |\n","| 3 | 0.737 | 0.767 |\n","| 3.5 | 0.725 | 0.744 |\n","| 4 | 0.719 | 0.721 |"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xP5aN_wXLELQ"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b25edbb95004874b39b5a849654281e","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Include a general discussion about what you did/learned above.'), Textarea(value=\"â€¦"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"671a9e415cad483fb97e144cb4864c57","version_major":2,"version_minor":0},"text/plain":["Button(description='Save Answers', style=ButtonStyle())"]},"metadata":{},"output_type":"display_data"},{"data":{"text/markdown":["**Include a general discussion about what you did/learned above.**\n","\n","I loaded in the glass dataset, dropped the ID column (since it has unique values for each row), and normalized the rest of the features. Then I split the data into train and test sets, and ran the model on three random splits to get an average accuracy. With the default values, the train and test accuracy was a little bit better than random, but not by much. I also printed out the probabilites one time, and the voting method was clear. Each probability is a multiple of 0.2, so each closest neighbor gave 0.2 votes, and the highest one won. I then experimented with different p values. The p value didn't seem to effect the training set much, with accuracies bouncing between 0.708 and 0.737, but the testing set accurcy did consistently decrease as the p value increased. This suggests that for this dataset, a lower p value leads to better generalization."],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["# @title 1.1 Discussion { display-mode: \"form\" }\n","# PLEASE DO NOT ALTER THIS CODE\n","if net_id == \"\":\n","  raise Exception(\"You need to set your net_id, silly goose.\")\n","discussion_id = \"1.1KNN\"\n","questions = [\"Include a general discussion about what you did/learned above.\"]\n","Discussion(discussion_id, questions, net_id); pass"]},{"cell_type":"markdown","metadata":{"id":"9vWiTdlbR2Xh"},"source":["## 2 KNN Classification with normalization and distance weighting\n","\n","Use the [magic telescope](https://axon.cs.byu.edu/data/uci_class/MagicTelescope.arff) dataset\n","\n","### 2.1 (5%) - Without Normalization or Distance Weighting\n","- Do random 80/20 train/test splits each time\n","- Run with k=3 and *without* distance weighting and *without* normalization\n","- Show train and test set accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4SSoasDQSKXb"},"outputs":[],"source":["# Learn magic telescope data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZNDp0udLELQ"},"outputs":[],"source":["# @title 2.1 Discussion { display-mode: \"form\" }\n","# PLEASE DO NOT ALTER THIS CODE\n","if net_id == \"\":\n","  raise Exception(\"You need to set your net_id, silly goose.\")\n","discussion_id = \"2.1KNN\"\n","questions = [\"Include a general discussion about what you did/learned above.\"]\n","Discussion(discussion_id, questions, net_id); pass"]},{"cell_type":"markdown","metadata":{"id":"q9h78s1KLELQ"},"source":["### 2.2 (10%) With Normalization\n","- Try it with k=3 without distance weighting but *with* normalization of input features.  You may use any reasonable normalization approach (e.g. standard min-max normalization between 0-1, z-transform, etc.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KbUW52yWLELR"},"outputs":[],"source":["# Train/Predict with normalization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMDOk5D9LELR"},"outputs":[],"source":["# @title 2.2 Discussion { display-mode: \"form\" }\n","# PLEASE DO NOT ALTER THIS CODE\n","if net_id == \"\":\n","  raise Exception(\"You need to set your net_id, silly goose.\")\n","discussion_id = \"2.2KNN\"\n","questions = [\"Discuss the results of using normalized data vs. unnormalized data.\", \"Why is it a good idea to normalize data before using KNN?\"]\n","Discussion(discussion_id, questions, net_id); pass"]},{"cell_type":"markdown","metadata":{"id":"Otxs23tuLELR"},"source":["### 2.3 (10%) With Distance Weighting\n","- Try it with k=3 and with distance weighting *and* normalization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3OAiCOPaLELR"},"outputs":[],"source":["#Train/Precdict with normalization and distance weighting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"69MWqVYhLELR"},"outputs":[],"source":["# @title 2.3 Discussion { display-mode: \"form\" }\n","# PLEASE DO NOT ALTER THIS CODE\n","if net_id == \"\":\n","  raise Exception(\"You need to set your net_id, silly goose.\")\n","discussion_id = \"2.3KNN\"\n","questions = [\"How did the results change when you used distance weighting?\"]\n","Discussion(discussion_id, questions, net_id); pass"]},{"cell_type":"markdown","metadata":{"id":"xTJLaqJVLELR"},"source":["### 2.4 (10%) Different k Values\n","- Using your normalized data with distance weighting, create one graph with classification accuracy on the test set on the y-axis and k values on the x-axis.\n","- Use values of k from 1 to 15.  Use the same train/test split for each."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwvYBraOLELR"},"outputs":[],"source":["# Calculate and Graph classification accuracy vs k values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hztLJv9pLELR"},"outputs":[],"source":["# @title 2.4 Discussion { display-mode: \"form\" }\n","# PLEASE DO NOT ALTER THIS CODE\n","if net_id == \"\":\n","  raise Exception(\"You need to set your net_id, silly goose.\")\n","discussion_id = \"2.4KNN\"\n","questions = [\"Which is the best k value for the `magic_telescope` dataset?\", \"Interpret/describe your graph.\"]\n","Discussion(discussion_id, questions, net_id); pass"]},{"cell_type":"markdown","metadata":{"id":"SIRG42TgSR4x"},"source":["## 3 KNN Regression with normalization and distance weighting\n","\n","Use the [sklean KNeighborsRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor) on the [housing price prediction](https://axon.cs.byu.edu/data/uci_regression/housing.arff) problem.  \n","### 3.1 (5%) Ethical Data\n","Note this data set has an example of an inappropriate input feature which we discussed.  State which feature is inappropriate and discuss why."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aGmBOd4rLELR"},"outputs":[],"source":["# @title 3.1 Discussion { display-mode: \"form\" }\n","# PLEASE DO NOT ALTER THIS CODE\n","if net_id == \"\":\n","  raise Exception(\"You need to set your net_id, silly goose.\")\n","discussion_id = \"3.1KNN\"\n","questions = [\"Which feature is innapropriate and why?\"]\n","Discussion(discussion_id, questions, net_id); pass"]},{"cell_type":"markdown","metadata":{"id":"UpbE6Y3PLELR"},"source":["### 3.2 (15%) - KNN Regression\n","- Do random 80/20 train/test splits each time\n","- Run with k=3\n","- Print the score (coefficient of determination) and Mean Absolute Error (MAE) for the train and test set for the cases of\n","  - No input normalization and no distance weighting\n","  - Normalization and no distance weighting\n","  - Normalization and distance weighting\n","- Normalize inputs features where needed but do not normalize the output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KBGUn43ASiXW"},"outputs":[],"source":["# Learn and experiment with housing price prediction data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I4o2Gw_ULELR"},"outputs":[],"source":["# @title 3.2 Discussion { display-mode: \"form\" }\n","# PLEASE DO NOT ALTER THIS CODE\n","if net_id == \"\":\n","  raise Exception(\"You need to set your net_id, silly goose.\")\n","discussion_id = \"3.2KNN\"\n","questions = [\"Discuss your results.\", \"Which method was the best?\"]\n","Discussion(discussion_id, questions, net_id); pass"]},{"cell_type":"markdown","metadata":{"id":"guxSgNNLLELS"},"source":["### 3.3 (10%)  Different k Values\n","- Using housing with normalized data and distance weighting, create one graph with MAE on the test set on the y-axis and k values on the x-axis\n","- Use values of k from 1 to 15.  Use the same train/test split for each."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lBdgJgFoLELS"},"outputs":[],"source":["# Learn and graph for different k values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XturKuI9LELS"},"outputs":[],"source":["# @title 3.3 Discussion { display-mode: \"form\" }\n","# PLEASE DO NOT ALTER THIS CODE\n","if net_id == \"\":\n","  raise Exception(\"You need to set your net_id, silly goose.\")\n","discussion_id = \"3.3KNN\"\n","questions = [\"Which is the best k value for the `housing` dataset?\", \"Interpret/describe your graph.\"]\n","Discussion(discussion_id, questions, net_id); pass"]},{"cell_type":"markdown","metadata":{"id":"1Im2ZI9OLELS"},"source":["## 4. (20%) KNN with nominal and real data\n","\n","- Use the [lymph dataset](https://axon.cs.byu.edu/data/uci_class/lymph.arff)\n","- Use a 80/20 split of the data for the training/test set\n","- This dataset has both continuous and nominal attributes\n","- Implement a distance metric which uses Euclidean distance for continuous features and 0/1 distance for nominal. Hints:\n","    - Write your own distance function (e.g. mydist) and use clf = KNeighborsClassifier(metric=mydist)\n","    - Change the nominal features in the data set to integer values since KNeighborsClassifier expects numeric features. I used Label_Encoder on the nominal features.\n","    - Keep a list of which features are nominal which mydist can use to decide which distance measure to use\n","    - There was an occasional bug in SK version 1.3.0 (\"Flags object has no attribute 'c_contiguous'\") that went away when I upgraded to the lastest SK version 1.3.1\n","- Use your own choice for k and other parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_IgmItkPLELS"},"outputs":[],"source":["# Train/Predict lymph with your own distance metric"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I1GS-VWULELS"},"outputs":[],"source":["# @title 4 Discussion { display-mode: \"form\" }\n","# PLEASE DO NOT ALTER THIS CODE\n","if net_id == \"\":\n","  raise Exception(\"You need to set your net_id, silly goose.\")\n","discussion_id = \"4KNN\"\n","questions = [\"Explain your distance metric.\", \"Discuss the results of using your own distance metric.\"]\n","Discussion(discussion_id, questions, net_id); pass"]},{"cell_type":"markdown","metadata":{"id":"Wxsc4khoLELS"},"source":["## 5. (Optional 15% extra credit) Code up your own KNN Learner\n","Below is a scaffold you could use if you want. Requirements for this task:\n","- Your model should support the methods shown in the example scaffold below\n","- Use Euclidean distance to decide closest neighbors\n","- Implement both the classification and regression versions\n","- Include optional distance weighting for both algorithms\n","- Run your algorithm on the magic telescope and housing data sets above and discuss and compare your results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kic1n6qvLELS"},"outputs":[],"source":["# @title 5 Discussion { display-mode: \"form\" }\n","# PLEASE DO NOT ALTER THIS CODE\n","if net_id == \"\":\n","  raise Exception(\"You need to set your net_id, silly goose.\")\n","discussion_id = \"5KNN\"\n","questions = [\"Discuss what you learned from implementing a KNN from scratch.\"]\n","Discussion(discussion_id, questions, net_id); pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"83tYJz-JLELS"},"outputs":[],"source":["from sklearn.base import BaseEstimator, ClassifierMixin\n","\n","class KNNClassifier(BaseEstimator,ClassifierMixin):\n","    def __init__(self, columntype=[], weight_type='inverse_distance'): ## add parameters here\n","        \"\"\"\n","        Args:\n","            columntype for each column tells you if continues[real] or if nominal[categoritcal].\n","            weight_type: inverse_distance voting or if non distance weighting. Options = [\"no_weight\",\"inverse_distance\"]\n","        \"\"\"\n","        self.columntype = columntype #Note This won't be needed until part 5\n","        self.weight_type = weight_type\n","\n","    def fit(self, data, labels):\n","        \"\"\" Fit the data; run the algorithm (for this lab really just saves the data :D)\n","        Args:\n","            X (array-like): A 2D numpy array with the training data, excluding targets\n","            y (array-like): A 2D numpy array with the training targets\n","        Returns:\n","            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n","        \"\"\"\n","        return self\n","\n","    def predict(self, data):\n","        \"\"\" Predict all classes for a dataset X\n","        Args:\n","            X (array-like): A 2D numpy array with the training data, excluding targets\n","        Returns:\n","            array, shape (n_samples,)\n","                Predicted target values per element in X.\n","        \"\"\"\n","        pass\n","\n","    #Returns the Mean score given input data and labels\n","    def score(self, X, y):\n","        \"\"\" Return accuracy of model on a given dataset. Must implement own score function.\n","        Args:\n","            X (array-like): A 2D numpy array with data, excluding targets\n","            y (array-like): A 2D numpy array with targets\n","        Returns:\n","            score : float\n","                Mean accuracy of self.predict(X) wrt. y.\n","        \"\"\"\n","        return 0"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.7 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
